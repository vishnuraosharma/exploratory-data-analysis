{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **Predicting Kilocalories Based on Nutritional Information**\n",
    "*Exploratory Data Analysis* | *Feature Selection* | *Model Building* | *Model Evaluation* | *Python* | *Linear Regression*\n",
    "\n",
    "* Author: Vishnu Rao-Sharma\n",
    "* Published: Fall 2023\n",
    "\n",
    "## Abstract\n",
    "With this dataset, I intend to find the relationship between macro/micro nutrients and Kilocalories in food. I will use SHRUTI SAXENA's [Food Nutrition Dataset](https://www.kaggle.com/datasets/shrutisaxena/food-nutrition-dataset?resource=download&select=food.csv) which is based on the USDA's [FoodData Central database](https://fdc.nal.usda.gov/fdc-app.html#/) to build a model that can predict the calories in a food based on key nutritional information."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5d53845daebd510"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b8770c52a7dac81"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\r\n"
     ]
    }
   ],
   "source": [
    "# Addding the dataset from github https://github.com/vraosharma-northeastern/exploratory-data-analysis/blob/main/Nutrition%20/food.csv\n",
    "!wget https://raw.githubusercontent.com/vraosharma-northeastern/exploratory-data-analysis/main/Nutrition%20/food.csv"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T22:08:34.943135Z",
     "start_time": "2023-10-25T22:08:34.810266Z"
    }
   },
   "id": "2b8db8d27b295380"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Importing the necessary libraries\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mstatsmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01msm\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# Importing the necessary libraries\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "from scipy.stats import shapiro\n",
    "from statistics import LinearRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "from mlxtend.evaluate import bias_variance_decomp"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T22:13:35.492544Z",
     "start_time": "2023-10-25T22:13:35.454127Z"
    }
   },
   "id": "a6bbd0e84b241abf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset Overview & Cleanup\n",
    "In this section we will evaluate the hygiene of the dataset and clean it up as necessary"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6278602b4bfc0f9d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Reading the file into a dataframe and viewing the first few rows\n",
    "df = pd.read_csv('food.csv')\n",
    "\n",
    "# Loop through columns and remove redundant 'Data.' tag from column names\n",
    "for col in df.columns:\n",
    "    new_col = col.replace('Data.', '')  # Remove 'Data.' from the column name\n",
    "    df.rename(columns={col: new_col}, inplace=True)  # Rename the column name in dataframe\n",
    "\n",
    "# Show the first few rows of the dataset\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T22:08:34.942363Z"
    }
   },
   "id": "9ce8d05ce6f9b83f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see above, each row represents a food type; the *Description* column contains a breakdown of the few ingredients that comprise the dish or food as well as details of how it was prepared. Some foods have multiple entries in the dataset because they were prepared in different ways, e.g. raw, cooked, fried, etc. Additionally, some foods have multiple entries because they were prepared with different ingredients, e.g. with or without salt, with or without sugar, etc.\n",
    "\n",
    "The consistency of the data also presents a challenge. For example, the *Category* column contains a few entries that are not necessarily food types, e.g. 'Spices and Herbs', 'no category', etc. and occassionally foods of the same type are split into categories by brand e.g. 'Soup', 'Campbell's Soup'. I am not sure if this is intentional or not, but I will leave this data as is for now. To clean this up, I could use GPT to classify the foods into categories, but that is outside the scope of this project."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f614cb0c7c34be53"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**What are the data types?**: Below, we can see the data types in this dataset are object (string), float64, and int64."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "426ee015e8d23bbb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Show data types and count of attributes per that type\n",
    "df.dtypes.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T22:08:34.943654Z"
    }
   },
   "id": "899c5c3c4b1a5f98"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Are there missing values?**: As we can see below, there are only null values in the two 'Household Weight Description' columns."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "233875a910353b61"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Find percentage of null values and sort by missing %\n",
    "df.isnull().sum().sort_values(ascending=False) / len(df) * 100"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T22:08:34.945920Z",
     "start_time": "2023-10-25T22:08:34.945130Z"
    }
   },
   "id": "4eec485e96e35ea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because *2nd Household Weight* and *1st Household Weight* are the same measure expressed in different units and this column has more nulls, we will drop *2nd Household Weight* and it's corresponding *Household Weights.2nd Household Weight Description* below.\n",
    "\n",
    "Also, because Household Weight Description doesn't really tell us anything that the *Household Weights.1st Household Weight* already containes, we will drop *Household Weights.1st Household Weight Description* as well."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "523c0661d76b73f5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Drop 'Household Weight Description' columns\n",
    "df.drop(['Household Weights.2nd Household Weight Description', 'Household Weights.2nd Household Weight','Household Weights.1st Household Weight Description'], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T22:08:34.947040Z",
     "start_time": "2023-10-25T22:08:34.946597Z"
    }
   },
   "id": "61f2e6ebade2ce59"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Despite having only non-null values, both categorical and numeric attributes of this dataset have a few \"placeholder\" values. Let's quantify these.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36c210e313727944"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Find the number of values in the 'Category' column that ci match '%no category%'\n",
    "df[df['Category'].str.contains('no category', case=False)]['Category'].count()/ len(df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T22:08:34.957040Z",
     "start_time": "2023-10-25T22:08:34.948218Z"
    }
   },
   "id": "b2dc268056f630ef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the *Category* column has less than 3% of its values labeled 'no category'. This is not a significant amount of data. Because we're trying to predict Kilocalories, and we don't care too much about the taxonomy of food types, we will keep these records."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "908123af72d1d15c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now target the numeric columns. We will look for values that are 0, which are likely placeholders for missing data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "507f0b5a22f2be1f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Show the percentage of 0s in each numeric column of the dataset. In another table, show the percentage of 1s in each numeric column of the dataset.\n",
    "\n",
    "df.select_dtypes(include=['float64', 'int64']).apply(lambda x: x[x == 0].count() / len(df)).sort_values(ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T22:08:34.949691Z"
    }
   },
   "id": "873c32fec9f8b395"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because we're dealing with a variety of foods, it makes sense that certain nutrients would be absent, so we will not impute any of these values. However, all foods should have a Weight (g) greater than 0. We will remove all rows where the Weight (g) is 0."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c25bf6016a3672eb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Drop rows where Household Weights.1st Household Weight is 0\n",
    "df = df[df['Household Weights.1st Household Weight'] != 0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T22:08:34.951161Z"
    }
   },
   "id": "f9d2c3e009df46d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, let's limit the scope of our dataset to exclude information that gives is found in small quantities in food. We will remove the following columns to simplify our analysis because we don't care about their correlation to KCAL:\n",
    "1. **Alpha Carotene**\n",
    "2. **Beta Carotene**\n",
    "3. **Beta Cryptoxanthin**\n",
    "4. **Lycopene**\n",
    "5. **Lutein and Zeaxanthin**\n",
    "6. **Retinol**\n",
    "7. **Riboflavin**\n",
    "8. **Selinium**\n",
    "9. **Thiamin**\n",
    "10. **Choline**\n",
    "11. **Manganese**\n",
    "12. **Niacin**\n",
    "13. **Pantothenic Acid**\n",
    "14. **Refuse Percentage**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65adeac95f12222f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Drop columns that are not relevant to our analysis\n",
    "df.drop(['Alpha Carotene', 'Beta Carotene', 'Beta Cryptoxanthin', 'Lycopene', 'Lutein and Zeaxanthin', 'Retinol', 'Riboflavin', 'Selenium', 'Thiamin', 'Choline', 'Manganese', 'Niacin', 'Pantothenic Acid', 'Refuse Percentage'], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T22:08:34.953346Z"
    }
   },
   "id": "13bae5ef3a44e769"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will also drop *Vitamin A - IU* because it is redundant to *Vitamin A - RAE* and the [NIH](https://ods.od.nih.gov/factsheets/VitaminA-HealthProfessional/#:~:text=The%20units%20of%20measurement%20for,beta%2Dcarotene%20%3D%200.3%20mcg%20RAE) recommends using *Vitamin A - RAE* to measure Vitamin A intake."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c99dc8e615db20b2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Drop *Vitamin A - IU* because it is redundant with *Vitamin A - RAE*\n",
    "df.drop(['Vitamins.Vitamin A - IU'], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T22:08:34.954992Z"
    }
   },
   "id": "b2356a9da2a6849f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Documentation for the dataset is a bit weak. For example, though the first few attributes have clearly defined units in Kaggle, the units for the remaining numeric attributes are not mentioned. Using the source of the data, the USDA FoodData Central, we can compare the values in our dataset to the source of truth and assume the units for each column. To make this simple, we will take the dataset's first entry, Butter [1001](https://fdc.nal.usda.gov/fdc-app.html#/food-details/790508/nutrients). We will assume that the units for each column are the same as the units in the source of truth unless values are off by order(s) of magnitude:\n",
    "\n",
    "**Attribute: Unit**\n",
    "1. Ash: g\n",
    "2. Carbohydrate: g\n",
    "3. Cholesterol: mg\n",
    "4. Fat.Monosaturated Fat: g\n",
    "5. Fat.Polysaturated Fat: g\n",
    "6. Fat.Saturated Fat: g\n",
    "7. Fat.Total Lipid: g\n",
    "8. Fiber: g\n",
    "9. Household Weights.1st Household Weight: g\n",
    "10. Kilocalories: kcal\n",
    "11. Major Minerals.Calcium: mg\n",
    "12. Major Minerals.Copper: mg\n",
    "13. Major Minerals.Iron: mg\n",
    "14. Major Minerals.Magnesium: mg\n",
    "15. Major Minerals.Phosphorus: mg\n",
    "16. Major Minerals.Potassium: mg\n",
    "17. Major Minerals.Sodium: mg\n",
    "18. Major Minerals.Zinc: mg\n",
    "19. Protein: g\n",
    "20. Sugar Total: g\n",
    "21. Vitamins.Vitamin A - RAE: µg\n",
    "22. Vitamins.Vitamin B12: µg\n",
    "23. Vitamins.Vitamin B6: µg\n",
    "24. Vitamins.Vitamin C: µg\n",
    "25. Vitamins.Vitamin E: mg\n",
    "26. Vitamins.Vitamin K: µg\n",
    "27. Water: g"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebaa8cde067d5817"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Show the first row of the dataset in alphabetical order\n",
    "df.sort_index(axis=1).head(1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T22:08:34.956197Z"
    }
   },
   "id": "5f7492726d1f0ab1"
  },
  {
   "cell_type": "markdown",
   "source": [
    " **Which independent variables have missing data? How much?**: Now that we've cleaned the dataset of irrelevant data, let's look at the missing values again. We can see below that there are no missing values. Remember, we have those  'no category' records in the *Category* column (3%)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "612dfc7a0e45d87f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Find percentage of null values and sort by missing %\n",
    "df.isnull().sum().sort_values(ascending=False) / len(df) * 100"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T22:08:34.959318Z",
     "start_time": "2023-10-25T22:08:34.957497Z"
    }
   },
   "id": "678e7afa394c7f25"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**What are the likely distributions of the numeric variables?**: Now that we've cleaned up the dataset, let's see the distributions of the numeric variables. We will use a histogram to visualize the distributions of the numeric variables."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "153bc7cfaa2f1a9b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Profiling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6c9087dd2b398d5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add the numeric columns excluding Nutrient Data Bank Number to a list\n",
    "numeric_cols = df.select_dtypes(include=['float64', 'int64']).drop(['Nutrient Data Bank Number'], axis=1)\n",
    "\n",
    "#create a histogram for each numeric column\n",
    "for column in numeric_cols.columns:\n",
    "    sns.set(rc={\"figure.figsize\": (8, 4)})\n",
    "    sns.displot(df[column])\n",
    "    plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T22:08:34.958875Z"
    }
   },
   "id": "9363635377aeec97"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the distributions of the numeric variables are very right-tailed. This is expected - for example, some foods are very rich in Vitamin A like Cod Liver, while others have none.\n",
    "\n",
    "The only variable that looks roughly normal is *Water*. Let's perform the Shapiro-Wilk test to verify if Water's or any other of the distributions are normal."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f158da65ab29a24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Perform the Shapiro-Wilk test on each numeric column excluding Nutrient Data Bank Number\n",
    "for column in numeric_cols.columns:\n",
    "    stat, p = shapiro(df[column])\n",
    "    print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "    # interpret\n",
    "    alpha = 0.05\n",
    "    if p > alpha:\n",
    "        print('{} looks Normal (fail to reject H0)'.format(column))\n",
    "    else:\n",
    "        print('{} does not look Normal (reject H0)'.format(column))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T22:08:34.960178Z"
    }
   },
   "id": "d8ff79e4d9099c04"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see above, none of the distributions are normal. This is expected because the distributions are right-tailed. We will not transform the data to make it normal because we are not using any models that require normality.\n",
    "\n",
    "***Note to Reader*** - I am not sure if the Shapiro-Wilk test is the perfect test to use here. Also, I attempted to show QQ plots for this data to understand if the distributions were another common distribution, but I was not able to tell."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42bf25e7b4220b01"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Selection\n",
    "Now that we have an idea of what we're working with, let's see which columns are the most useful for predicting Kilocalories. We will use three methods to determine which features are the most useful:\n",
    "1. **Pearson's Correlation**: We will use Pearson's Correlation to determine which features are the most correlated to Kilocalories. We will use a threshold of 0.5.\n",
    "2. **Sequential Feature Selection**: We will use Sequential Feature Selection to determine which features are the most useful. We will use a linear regression model to determine the best features. We will use a step size of 1 and a cross-validation of 5.\n",
    "3. **Variance Thresholding**: We will use Variance Thresholding to determine which features are the most useful. We will use a threshold of 0.2."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f7c04872860800c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Which independent variables are useful to predict a target (dependent variable)? (Use at least three methods)**\n",
    "\n",
    "First up, let's use ***Pearson's Correlation*** look at the relationships between *Kilocalories* and the other numeric variables."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b110a308e751c9a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a copy of the dataset\n",
    "df_fs_correlation = df.copy()\n",
    "\n",
    "# Only keep feature candidates\n",
    "df_fs_correlation = df_fs_correlation.select_dtypes(include=['float64', 'int64']).drop(['Nutrient Data Bank Number'], axis=1)\n",
    "\n",
    "# Create a correlation matrix of all independent variables\n",
    "corr_matrix = df_fs_correlation.corr()\n",
    "\n",
    "# Create a heatmap of all independent variables\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T22:08:34.962094Z"
    }
   },
   "id": "1b322c0220b24e94"
  },
  {
   "cell_type": "markdown",
   "source": [
    "A few things jump out at us from the heatmap that will help us narrow down our features:\n",
    "1.  *Fat.Total Lipid* and the other sub-types of fat (e.g. *Monounsaturated*, *Saturated*) are highly correlated to each other. This makes sense because Total Lipid is the sum of the other sub-types of fat. Because we are trying to predict *Kilocalories*, we will keep *Fat.Total Lipid* and drop the other sub-types of fat. The high correlation shows us that Total Fat will likely be our best predictor of Calories and this will help mitigate the multi-collinearity between the sub-types of fat.\n",
    "2. Water's negative correlation to *Kilocalories* will likely make it a great feature to keep. Because foods like vegetables are mostly water and have few calories this makes sense.\n",
    "3. Major Minerals seem to be clustered together as well, for example *Calcium* and *Phosphorus* are highly correlated. None of these minerals are highly correlated to *Kilocalories*, so we will drop them.\n",
    "4. Interestingly *Sodium* and *Ash* are highly correlated. The relationship between these two is not clear. Because we dropped Sodium in the previous step, we will keep Ash.\n",
    "5. Vitamins in this dataset appear to be clustered together as well. None of these vitamins are highly correlated to *Kilocalories*, with an exception of *Vitamin E*. In order to satisfy the requirements of the assignment to use a categorical variable, we will introduce a new feature called *Vitamin E Present* that will be a binary variable indicating whether or not a food contains Vitamin E."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bea30b9628cd8494"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a dummy column called 'Vitamin E Present' that is 1 if Vitamin E is present and 0 if it is not\n",
    "df['Vitamin E Present'] = np.where(df['Vitamins.Vitamin E'] > 0, 1, 0)\n",
    "\n",
    "# Create a dummy column called 'Vitamin E Present' that is 1 if Vitamin E is present and 0 if it is not\n",
    "df_fs_correlation['Vitamin E Present'] = np.where(df_fs_correlation['Vitamins.Vitamin E'] > 0, 1, 0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T22:08:34.963720Z"
    }
   },
   "id": "895e7dcf23d57ce3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Drop columns that Pearson's test highlighted as not useful above\n",
    "df_fs_correlation.drop(['Fat.Monosaturated Fat', 'Fat.Polysaturated Fat', 'Fat.Saturated Fat', 'Major Minerals.Calcium', 'Major Minerals.Copper', 'Major Minerals.Iron', 'Major Minerals.Magnesium', 'Major Minerals.Phosphorus', 'Major Minerals.Potassium', 'Major Minerals.Sodium', 'Major Minerals.Zinc', 'Vitamins.Vitamin A - RAE', 'Vitamins.Vitamin B12', 'Vitamins.Vitamin B6', 'Vitamins.Vitamin C', 'Vitamins.Vitamin E', 'Vitamins.Vitamin K'], axis=1, inplace=True)\n",
    "\n",
    "# Show our current Feature Set after analysis of Pearson's Correlation\n",
    "df_fs_correlation.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T22:08:34.965357Z"
    }
   },
   "id": "132db1a234a22aa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, let's use ***Sequential Feature Selection*** to select the best features. We will use a step size of 1 and a cross-validation of 5."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fac29dd78a0dc719"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a copy of the dataset\n",
    "df_fs_SFS = df.copy()\n",
    "df_fs_SFS = df_fs_SFS.select_dtypes(include=['float64', 'int64']).drop(['Nutrient Data Bank Number'], axis=1)\n",
    "\n",
    "X = df_fs_SFS.drop(['Kilocalories'], axis=1)\n",
    "y = df_fs_SFS[['Kilocalories']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Initialize the SequentialFeatureSelector\n",
    "sfs = SFS(model,\n",
    "          k_features='best',  # Select the best number of features automatically\n",
    "          forward=True,  # Perform forward feature selection\n",
    "          floating=False,  # No floating feature selection\n",
    "          scoring='neg_mean_squared_error',  # Scoring metric (RMSE in this case)\n",
    "          cv=5  # Cross-validation folds\n",
    "          )\n",
    "\n",
    "# Fit the SFS object to the training data\n",
    "sfs = sfs.fit(X_train, y_train)\n",
    "\n",
    "# Get the selected feature indices and names\n",
    "selected_feature_indices = sfs.k_feature_idx_\n",
    "selected_feature_names = X.columns[list(selected_feature_indices)]\n",
    "\n",
    "# Get the best subset of features\n",
    "best_subset = X_train.iloc[:, list(selected_feature_indices)]\n",
    "\n",
    "# Fit the model with the best subset of features\n",
    "model.fit(best_subset, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test[selected_feature_names])\n",
    "\n",
    "# Calculate RMSE on the test set\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE with selected features: {rmse}\")\n",
    "\n",
    "# Print the selected feature names\n",
    "print(\"Selected Features:\", selected_feature_names)\n",
    "\n",
    "# Update df_fs_SFS to only include the selected features\n",
    "df_fs_SFS = df_fs_SFS[selected_feature_names]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T22:08:34.967072Z"
    }
   },
   "id": "9e435dcdb3538c19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig1 = plot_sfs(sfs.get_metric_dict(), kind='std_dev')\n",
    "plt.title('Sequential Forward Selection')\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T22:08:34.968330Z"
    }
   },
   "id": "db574701c31ee1ae"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see above, the Sequential Feature Selection method resulted in similar results to the Pearson's Correlation method. That said:\n",
    "1. Again, we should follow the principle above to drop the sub-types of fat and keep *Fat.Total Lipid*.\n",
    "2. This method indicates that micronutrients like *Vitamin B6* and *Copper*  may have a larger impact on *Kilocalories* than the others, but we will drop them all (aside from our dummy var *Vitamin E* present) because it will affect the interpretibility of our model - it may lead us to false conclusions (e.g. high Copper foods are super low cal!). Keeping some of these micronutrients may also cause multi-collinearity issues with the other micronutrients.\n",
    "3. Interestingly, this method indicated that *Protein* may not be a good predictor of *Kilocalories*. This makes sense because high protein foods, unlike high fat foods tend to be lower in calories.\n",
    "4. We see *Cholesterol* show up here, which could de-stabilize our model given its correlation to *Fat.Total Lipid*. We will keep it for now because it is a significant predictor of *Kilocalories* according to both methods."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5d49368abf1ae5a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, let's use ***Variance Thresholding***. This is a simple method that removes features whose variance does not meet a certain threshold. Given our right-tailed histograms, we should take this step to ensure our features are decently spread out. We will use a threshold of 0.2.\n",
    "\n",
    "Because we have a good idea of what our features are from the methods above, we will use the intersection of the features from the Pearson's Correlation method and the Sequential Feature Selection method and exclude anything that doesn't meet our threshold."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc9555d5aba56baa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a VarianceThreshold object with a threshold of 0.2\n",
    "vt = VarianceThreshold(threshold=0.2)\n",
    "\n",
    "# Create copy of our df for feature selection\n",
    "df_fs_vt = df.copy()\n",
    "\n",
    "# Only keep feature candidates\n",
    "df_fs_vt = df_fs_vt.select_dtypes(include=['float64', 'int64']).drop(['Nutrient Data Bank Number'], axis=1)\n",
    "\n",
    "# Fit the VarianceThreshold object to ds_fs_ffs\n",
    "vt.fit(df_fs_vt)\n",
    "\n",
    "# Create a boolean mask from the VarianceThreshold object\n",
    "mask = vt.get_support()\n",
    "\n",
    "# Create a reduced dataset by applying the mask to ds_fs_ffs\n",
    "reduced_ds_fs_ffs = df_fs_vt.loc[:, mask]\n",
    "\n",
    "# Create a list of features to keep\n",
    "df_selected_features = reduced_ds_fs_ffs.copy()\n",
    "\n",
    "# Show the list of features to keep\n",
    "df_selected_features.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T22:08:34.969720Z"
    }
   },
   "id": "73406e1ecb77f904"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This shows us that none of the features have a variance less than 0.2. None of the features will be excluded in this step.\n",
    "\n",
    "Our best features seem to be the ones we identified in the correlation method. Let's keep those and move on."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f08245745c3c8bfa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Our final features end up being the ones we identified in the correlation method\n",
    "df_selected_features = df_fs_correlation.copy()\n",
    "\n",
    "df_selected_features.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T22:08:34.970733Z"
    }
   },
   "id": "952862fb79abaff7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Selected Feature Significance\n",
    "With our selected features, let's now determine which features are the most significant. We will use the Ordinary Least Squares (OLS) method to determine the significance of each feature. We will use a significance level of 0.05."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "981d235a62e79214"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a model of the numeric features in df_fs_OLS with the target variable Kilocalories\n",
    "model = sm.OLS(df_selected_features['Kilocalories'], df_selected_features.drop(['Kilocalories'], axis=1)).fit()\n",
    "\n",
    "# Show the summary of the model\n",
    "print(model.summary())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T22:08:34.971471Z"
    }
   },
   "id": "bae2dd2d1593c97b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Which predictor variables are the most important?**\n",
    "Considering the P values of <0.05 tell us that the probability of a feature being significant is low, the OLS method tells us the following.\n",
    "\n",
    "1. Following our discovery in the correlation step, *Fat.Total Lipid* is the most important predictor of Kilocalories, followed by Protein and Carbohydrates. Of course, we expect our macronutrients to drive the majority of calories.\n",
    "2. When it comes to negative predictors of Kilocalories, *Fiber* overindexes other features. Fiborous vegetables are low in calories, so this makes sense.\n",
    "3. Surprisingly, *Water* is just barely significant. This is likely because the majority of foods are mostly water, so it is not a good predictor of Kilocalories. Also, foods like soups and stews are mostly water and have a lot of calories, so this may be why the coefficient is negative.\n",
    "4. Also unexpectedly, *Sugar Total* has a relatively low coefficient. This is likely because sugar is a type of carbohydrate and is already accounted for in the *Carbohydrate* feature.\n",
    "5. Our Dummy Variable for *Vitamin E Present* is not significant, so we can't say much about the impact one way or another based on this regression."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1be4b432806a9517"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Are the predictor variables independent of all the other predictor variables?**\n",
    "In the OLS summary above we can see that the Durbin-Watson statistic is less than one. This statistic tests for autocorrelation. The value of .987 is below 2, which [indicates that there is positive autocorrelation](https://www.investopedia.com/terms/d/durbin-watson-statistic.asp) in the data. This means that the predictor variables are probably correlated.\n",
    "\n",
    "As stated above, Carbohydrates and Sugar Total are obviously related. Similarly, Cholesterol and Fat.Total Lipid are related as fat contains cholesterol. We will keep these features in our model because they are significant predictors of Kilocalories."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3c440deac7c9427"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize the ranges of the features with boxplots\n",
    "df_selected_features.boxplot(rot=90, figsize=(20, 10))\n",
    "plt.show()\n",
    "# Show ranges of the features with boxplots\n",
    "df_selected_features.describe().T"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T22:08:34.972246Z"
    }
   },
   "id": "3bb1ed9a2874cdb9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Do the ranges of the predictor variables make sense?** A few things seem odd about the ranges of the features:\n",
    "1. The range of *Fat.Total Lipid*, *Water*, and *Carbohydrate* is 0-100. This is odd because fat is measured in grams and the range of each seem arbitrarily locked at 100.\n",
    "2. The Household Weight of each food is measured in grams, but the range max is 8399 g. This makes sense because the heaviest food in the dataset is 8399 g, which is 18.5 lbs. Though this is an outlier, the entry is for a Turkey which sounds about right. We can also see a huge number of outliers in the boxplot above. This makes sense as the dataset contains a variety of foods - from a stick of butter to an entire bird.\n",
    "\n",
    "**What are the distributions of the predictor variables?** The distributions of the predictor variables are right-tailed and not normal. As explained earlier, this is expected because not all foods have the same nutritional value.\n",
    "\n",
    "We can also see that our new dummy variable is very balanced with a mean of 0.54, meaning about half of foods contain Vitamin E."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "213080ea88b16638"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Building\n",
    "Now that we have our selected features, let's build a model to predict Kilocalories. We will use a Linear Regression model because we are predicting a continuous variable.\n",
    "\n",
    "From there, we'll make some tweaks to our training data to see if we can improve the model's performance as measured by RMSE and R^2."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15cab19734464a39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Using df_selected_features, split the data into training and test sets where 20% of the data is in the test set\n",
    "X = df_selected_features.drop(['Kilocalories'], axis=1)\n",
    "y = df_selected_features[['Kilocalories']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# For each feature, plot histograms of the feature in the training and test sets\n",
    "for column in X_train.columns:\n",
    "    sns.set(rc={\"figure.figsize\": (8, 4)})\n",
    "\n",
    "    # Create histograms for training data in orange with opacity\n",
    "    sns.histplot(X_train[column], color='orange', alpha=0.7, bins=30, label='Training Data')\n",
    "\n",
    "    # Create histograms for test data in green with opacity\n",
    "    sns.histplot(X_test[column], color='green', alpha=0.7, bins=30, label='Test Data')\n",
    "\n",
    "    plt.title(f'Histogram of {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T22:08:34.972990Z"
    }
   },
   "id": "dfb3b1da17f195ad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a linear regression model\n",
    "originalmodel = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "originalmodel.fit(X_train, y_train)\n",
    "\n",
    "# Let's take a look at the model's coefficient for each feature\n",
    "print('Model Coefficients:')\n",
    "for i in range(len(X_train.columns)):\n",
    "    print(f'{X_train.columns[i]}: {originalmodel.coef_[0][i]}')\n",
    "\n",
    "# Let's take a look at the model's intercept\n",
    "print(f'Intercept: {originalmodel.intercept_[0]}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T22:08:34.982549Z",
     "start_time": "2023-10-25T22:08:34.973712Z"
    }
   },
   "id": "c06433675f234444"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Let's take a look at the model's performance on the full dataset's test data\n",
    "# Make predictions on the test set\n",
    "y_pred = originalmodel.predict(X_test)\n",
    "\n",
    "# Plot RSME vs. Predicted Kilocalories\n",
    "sns.scatterplot(x=y_pred.flatten(), y=y_test.values.flatten())\n",
    "\n",
    "# Plot a line from (0,0) to (5000, 5000)\n",
    "plt.plot([0, 1500], [0, 1500], color='red', lw=.5)\n",
    "\n",
    "# Set the x and y axes labels\n",
    "plt.xlabel('Predicted Kilocalories')\n",
    "plt.ylabel('Actual Kilocalories')\n",
    "\n",
    "# Set the plot title and show the plot\n",
    "plt.title('Predicted vs. Actual Kilocalories')\n",
    "plt.show()\n",
    "\n",
    "# Calculate RMSE on the test set\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE with selected features: {rmse}\")\n",
    "\n",
    "# Calculate R^2 on the test set\n",
    "r2 = originalmodel.score(X_test, y_test)\n",
    "print(f\"R^2 with selected features: {r2}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T22:08:34.974458Z"
    }
   },
   "id": "18ce41a34521a9db"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The model's R^2 is 0.992, which is very high. This means that the model explains 99.2% of the variance in the data. This is likely means the model is overfitting the data. Our RSME is 15.7, which is not bad considering the range of Kilocalories is 0-902.\n",
    "\n",
    "Let's see what happens when we remove the outliers from the dataset and retrain."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57a208f5ac7bc320"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a copy of the dataset\n",
    "df_no_outliers = df_selected_features.copy()\n",
    "\n",
    "# For each feature, calculate the mean and standard deviation of the feature. Then, remove the entire record if outliers that are more than 3 standard deviations from the mean exist for a given feature.\n",
    "for column in df_no_outliers.columns:\n",
    "    if column != 'Kilocalories':\n",
    "        mean = df_selected_features[column].mean()\n",
    "        std = df_selected_features[column].std()\n",
    "        # For each row in df_no_outliers If the value is more than 3 standard deviations from the mean, remove the row from df_no_outliers\n",
    "        df_no_outliers = df_no_outliers[(df_no_outliers[column] - mean) / std < 3]\n",
    "\n",
    "# Show the shape of the dataset\n",
    "df_no_outliers.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T22:08:34.975824Z"
    }
   },
   "id": "f0e2dcc3126b4d70"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Using df_no_outliers, let's train our model on all of the outlier-free training data\n",
    "X = df_no_outliers.drop(['Kilocalories'], axis=1)\n",
    "y = df_no_outliers[['Kilocalories']]\n",
    "\n",
    "# We won't create a test set because we will use the same test set as before - we'll just create/use training data for our outlier free model\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Create a linear regression model\n",
    "model_no_outlier = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_no_outlier.fit(X_train_2, y_train_2)\n",
    "\n",
    "# Let's take a look at the model's coefficient for each feature\n",
    "print('Model Coefficients:')\n",
    "for i in range(len(X_train_2.columns)):\n",
    "    print(f'{X_train_2.columns[i]}: {model_no_outlier.coef_[0][i]}')\n",
    "\n",
    "# Let's take a look at the model's intercept\n",
    "print(f'Intercept: {model_no_outlier.intercept_[0]}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T22:08:34.976611Z"
    }
   },
   "id": "baf03658d161cd6c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Let's take a look at the model's performance on the same testing data from the full dataset\n",
    "# Make predictions on the test set\n",
    "y_pred = model_no_outlier.predict(X_test)\n",
    "\n",
    "# Plot RSME vs. Predicted Kilocalories\n",
    "sns.scatterplot(x=y_pred.flatten(), y=y_test.values.flatten())\n",
    "\n",
    "# Plot a line from (0,0) to (5000, 5000)\n",
    "plt.plot([0, 1000], [0, 1000], color='red', lw=0.5)\n",
    "\n",
    "# Set the x and y axes labels\n",
    "plt.xlabel('Predicted Kilocalories')\n",
    "plt.ylabel('Actual Kilocalories')\n",
    "\n",
    "# Set the plot title and show the plot\n",
    "plt.title('Predicted vs. Actual Kilocalories - Trained Without Outliers')\n",
    "plt.show()\n",
    "\n",
    "# Calculate RMSE on the test set\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE with selected features: {rmse}\")\n",
    "\n",
    "# Calculate R^2 on the test set\n",
    "r2 = model_no_outlier.score(X_test, y_test)\n",
    "print(f\"R^2 with selected features: {r2}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T22:08:34.977323Z"
    }
   },
   "id": "edfa9c0821780c44"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Remove outliers and keep outliers (does it have an effect of the final predictive model)?** Removing the outliers from the dataset did not have a significant effect on the model's performance. The model's R^2 is still 0.992, which is very high.\n",
    "\n",
    "Also, our RSME is 16.9, which is really not bad considering the range of Kilocalories is 0-902."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26169f1fc56e810b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Impact of Imputation\n",
    "Let's see what happens if we remove some random records from the dataset, impute those values, and retrain."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b635ce661aa8ed6f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a copy of the dataset\n",
    "df_1_per_random = df_selected_features.copy()\n",
    "df_5_per_random = df_selected_features.copy()\n",
    "df_10_per_random = df_selected_features.copy()\n",
    "\n",
    "# Calculate the number of cells in the dataset\n",
    "num_cells = df_1_per_random.shape[0] * df_1_per_random.shape[1]\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# For 1% of the cells in the dataset, randomly choose a row and column index and replace the value with -100000\n",
    "countMissing = df_1_per_random.isin([-100000]).sum().sum()\n",
    "while(countMissing < int(num_cells * 0.01)):\n",
    "    # Choose a random row index\n",
    "    row_index = np.random.randint(0, df_1_per_random.shape[0])\n",
    "\n",
    "    # Choose a random column index\n",
    "    col_index = np.random.randint(0, df_1_per_random.shape[1])\n",
    "\n",
    "    # If column is Kilocalories, skip and choose another column\n",
    "    if df_1_per_random.columns[col_index] == 'Kilocalories' or df_1_per_random.columns[col_index] == 'Vitamin E Present':\n",
    "        continue\n",
    "\n",
    "    # Replace the value with -100000\n",
    "    df_1_per_random.iloc[row_index, col_index] = -100000\n",
    "    countMissing = df_1_per_random.isin([-100000]).sum().sum()\n",
    "\n",
    "# For 5% of the cells in the dataset, randomly choose a row and column index and replace the value with -100000\n",
    "countMissing = df_5_per_random.isin([-100000]).sum().sum()\n",
    "while(countMissing < int(num_cells * 0.05)):\n",
    "    # Choose a random row index\n",
    "    row_index = np.random.randint(0, df_5_per_random.shape[0])\n",
    "\n",
    "    # Choose a random column index\n",
    "    col_index = np.random.randint(0, df_5_per_random.shape[1])\n",
    "\n",
    "     # If column is Kilocalories, skip and choose another column\n",
    "    if df_5_per_random.columns[col_index] == 'Kilocalories' or df_5_per_random.columns[col_index] == 'Vitamin E Present':\n",
    "        continue\n",
    "\n",
    "    # Replace the value with -100000\n",
    "    df_5_per_random.iloc[row_index, col_index] = -100000\n",
    "    countMissing = df_5_per_random.isin([-100000]).sum().sum()\n",
    "\n",
    "# For 10% of the cells in the dataset, randomly choose a row and column index and replace the value with -100000\n",
    "countMissing = df_10_per_random.isin([-100000]).sum().sum()\n",
    "while(countMissing < int(num_cells * 0.1)):\n",
    "    # Choose a random row index\n",
    "    row_index = np.random.randint(0, df_10_per_random.shape[0])\n",
    "\n",
    "    # Choose a random column index\n",
    "    col_index = np.random.randint(0, df_10_per_random.shape[1])\n",
    "\n",
    "    # If column is Kilocalories, skip and choose another column\n",
    "    if df_10_per_random.columns[col_index] == 'Kilocalories' or df_10_per_random.columns[col_index] == 'Vitamin E Present':\n",
    "        continue\n",
    "\n",
    "    # Replace the value with -100000\n",
    "    df_10_per_random.iloc[row_index, col_index] = -100000\n",
    "    countMissing = df_10_per_random.isin([-100000]).sum().sum()\n",
    "\n",
    "# Compare the count of ImputeMe values in each dataset\n",
    "print('1% Missing Values: {}'.format(df_1_per_random.isin([-100000]).sum().sum()))\n",
    "print('5% Missing Values: {}'.format(df_5_per_random.isin([-100000]).sum().sum()))\n",
    "print('10% Missing Values: {}'.format(df_10_per_random.isin([-100000]).sum().sum()))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T22:08:34.978071Z"
    }
   },
   "id": "8f8235e43699c663"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's try imputing the missing values of the random records we removed above. We will use three methods to replace the missing values:\n",
    "1. **Mean**: We will replace the missing values with the mean of the column. We will apply this to the 1% missing values dataset.\n",
    "2. **Median**: We will replace the missing values with the median of the column. We will apply this to the 5% missing values dataset.\n",
    "3. **Mode**: We will replace the missing values with the mode of the column. We will apply this to the 10% missing values dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b252d9dda87acea0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For each column in df_1_per_random, replace the -100000 values with the mean of the column in df_selected_features\n",
    "for column in df_1_per_random.columns:\n",
    "    # if the column is not Kilocalories\n",
    "    if column != 'Kilocalories':\n",
    "\n",
    "    # Calculate the mean of the column in df_selected_features\n",
    "        mean = df_selected_features[column].mean()\n",
    "\n",
    "        # Replace the -100000 values with the mean\n",
    "        df_1_per_random[column].replace(-100000, mean, inplace=True)\n",
    "\n",
    "# For each column in df_5_per_random, replace the -100000 values with the Median of the column in df_selected_features\n",
    "for column in df_5_per_random.columns:\n",
    "    # if the column is not Kilocalories\n",
    "    if column != 'Kilocalories':\n",
    "\n",
    "        # Calculate the Median of the column in df_selected_features\n",
    "        median = df_selected_features[column].median()\n",
    "\n",
    "        # Replace the -100000 values with the Median\n",
    "        df_5_per_random[column].replace(-100000, median, inplace=True)\n",
    "\n",
    "# For each column in df_10_per_random, replace the -100000 values with the Mode of the column in df_selected_features\n",
    "for column in df_10_per_random.columns:\n",
    "    # if the column is not Kilocalories\n",
    "    if column != 'Kilocalories':\n",
    "\n",
    "    # Calculate the Mode of the column in df_selected_features\n",
    "        mode = df_selected_features[column].mode()[0]\n",
    "\n",
    "        # Replace the -100000 values with the Mode\n",
    "        df_10_per_random[column].replace(-100000, mode, inplace=True)\n",
    "\n",
    "# Show the descriptions of our three imputed datasets\n",
    "df_1_per_random.describe().T"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T22:08:34.978771Z"
    }
   },
   "id": "a4f553b83b0ab160"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_5_per_random.describe().T"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T22:08:34.979427Z"
    }
   },
   "id": "748d89f9b3ba8e99"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_10_per_random.describe().T"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T22:08:34.980116Z"
    }
   },
   "id": "9404af51f8101a9d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Show the description of our original dataset\n",
    "df_selected_features.describe().T"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T22:08:34.980920Z"
    }
   },
   "id": "acb61ae7832cab41"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we've imputed the missing values, let's train some new models on our imputed datasets."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a98055dc04c3ee77"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Using df_1_per_random, let's train a model on mean-replaced data\n",
    "X = df_1_per_random.drop(['Kilocalories'], axis=1)\n",
    "y = df_1_per_random[['Kilocalories']]\n",
    "\n",
    "# We won't create a test set because we will use the same test set as before\n",
    "X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Create a linear regression model\n",
    "model_mean = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_mean.fit(X_train_3, y_train_3)\n",
    "\n",
    "# Let's take a look at the model's coefficient for each feature\n",
    "print('Model Coefficients:')\n",
    "for i in range(len(X_train_3.columns)):\n",
    "    print(f'{X_train_3.columns[i]}: {model_mean.coef_[0][i]}')\n",
    "\n",
    "# Let's take a look at the model's intercept\n",
    "print(f'Intercept: {model_mean.intercept_[0]}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T22:08:34.981598Z"
    }
   },
   "id": "b19cb79795de70a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Using df_5_per_random, let's train a model on median-replaced data\n",
    "X = df_5_per_random.drop(['Kilocalories'], axis=1)\n",
    "y = df_5_per_random[['Kilocalories']]\n",
    "\n",
    "# We won't create a test set because we will use the same test set as before\n",
    "X_train_4, X_test_4, y_train_4, y_test_4 = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Create a linear regression model\n",
    "model_median = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_median.fit(X_train_4, y_train_4)\n",
    "\n",
    "# Let's take a look at the model's coefficient for each feature\n",
    "print('Model Coefficients:')\n",
    "for i in range(len(X_train_4.columns)):\n",
    "    print(f'{X_train_4.columns[i]}: {model_median.coef_[0][i]}')\n",
    "\n",
    "# Let's take a look at the model's intercept\n",
    "print(f'Intercept: {model_median.intercept_[0]}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T22:08:34.982297Z"
    }
   },
   "id": "168f27d75f0fd42d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Using df_10_per_random, let's train a model on mode-replaced data\n",
    "X = df_10_per_random.drop(['Kilocalories'], axis=1)\n",
    "y = df_10_per_random[['Kilocalories']]\n",
    "\n",
    "# We won't create a test set because we will use the same test set as before\n",
    "X_train_5, X_test_5, y_train_5, y_test_5 = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Create a linear regression model\n",
    "model_mode = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_mode.fit(X_train_5, y_train_5)\n",
    "\n",
    "# Let's take a look at the model's coefficient for each feature\n",
    "print('Model Coefficients:')\n",
    "for i in range(len(X_train_5.columns)):\n",
    "    print(f'{X_train_4.columns[i]}: {model_mode.coef_[0][i]}')\n",
    "\n",
    "# Let's take a look at the model's intercept\n",
    "print(f'Intercept: {model_mode.intercept_[0]}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T22:08:35.105930Z",
     "start_time": "2023-10-25T22:08:34.982988Z"
    }
   },
   "id": "6d2912781dfa0b72"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's take a look at the bias and variance compared to our original model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3152c71cf7c7228"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a blank df to store our results with cols [Model, RMSE, R^2, Bias, Variance]\n",
    "df_results = pd.DataFrame(columns=['Model', 'MSE', 'Bias', 'Variance'])\n",
    "\n",
    "# Ensure that y_train and y_test are 1-dimensional arrays\n",
    "y_train = y_train.squeeze()\n",
    "y_test = y_test.squeeze()\n",
    "\n",
    "# Calculate bias and variance using mlxtend's bias_variance_decomp\n",
    "mse, bias, variance = bias_variance_decomp(originalmodel, X_train.values, y_train.values, X_test.values, y_test.values, loss='mse', num_rounds=200, random_seed=42)\n",
    "\n",
    "# Add the results to our df\n",
    "df_results = df_results.append({'Model': 'Original', 'MSE': mse, 'Bias': bias, 'Variance': variance}, ignore_index=True)\n",
    "\n",
    "# Check the bias and variance of our model trained on 1% mean-replaced data\n",
    "y_train_2 = y_train_2.squeeze()\n",
    "\n",
    "mse, bias, variance = bias_variance_decomp(model_mean, X_train_2.values, y_train_2.values, X_test.values, y_test.values, loss='mse', num_rounds=200, random_seed=42)\n",
    "\n",
    "# Add the results to our df\n",
    "df_results = df_results.append({'Model': 'Outliers Removed', 'MSE': mse, 'Bias': bias, 'Variance': variance}, ignore_index=True)\n",
    "\n",
    "# Check the bias and variance of our model trained on 1% mean-replaced data\n",
    "y_train_3 = y_train_3.squeeze()\n",
    "\n",
    "mse, bias, variance = bias_variance_decomp(model_mean, X_train_3.values, y_train_3.values, X_test.values, y_test.values, loss='mse', num_rounds=200, random_seed=42)\n",
    "\n",
    "# Add the results to our df\n",
    "df_results = df_results.append({'Model': '1% Mean', 'MSE': mse, 'Bias': bias, 'Variance': variance}, ignore_index=True)\n",
    "\n",
    "# Check the bias and variance of our model trained on 5% median-replaced data\n",
    "y_train_4 = y_train_4.squeeze()\n",
    "mse, bias, variance = bias_variance_decomp(model_median, X_train_4.values, y_train_4.values, X_test.values, y_test.values, loss='mse', num_rounds=200, random_seed=42)\n",
    "\n",
    "# Add the results to our df\n",
    "df_results = df_results.append({'Model': '5% Median', 'MSE': mse, 'Bias': bias, 'Variance': variance}, ignore_index=True)\n",
    "\n",
    "# Check the bias and variance of our model trained on 10% mode-replaced data\n",
    "y_train_5 = y_train_5.squeeze()\n",
    "mse, bias, variance = bias_variance_decomp(model_mode, X_train_5.values, y_train_5.values, X_test.values, y_test.values, loss='mse', num_rounds=200, random_seed=42)\n",
    "\n",
    "# Add the results to our df\n",
    "df_results = df_results.append({'Model': '10% Mode', 'MSE': mse, 'Bias': bias, 'Variance': variance}, ignore_index=True)\n",
    "\n",
    "# Show the results\n",
    "df_results\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T22:08:34.983711Z"
    }
   },
   "id": "1cc61c31c3312988"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**What is the impact of imputation on the model's performance? How well did the methods recover the missing values?** As we can see above, the imputation methods had a varying impact on the model's performance. All three imputation methods actually increased the model's bias and variance. Evidently, they are introducing noise into the dataset.\n",
    "\n",
    "When it comes to how well the methods recovered the missing values, the 1% Mean Model was the best with a close MSE and Variance. It's likely that this is because a small percent of the data was imputed, rather than an indication that this is the best method. Because our data is so skewed, it makes sense that the median would probably outperform the mean - with such a right-skewed dataset, the median will be closer to the true center of the data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc13abf042300e65"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot Model Performance by Model Type\n",
    "sns.barplot(x='Model', y='MSE', data=df_results)\n",
    "plt.title('Model Performance by Model Type')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T22:08:34.984465Z"
    }
   },
   "id": "5fbd188705396f32"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "Code is my own, but I used the following resources to help me along the way:\n",
    "1. [Kaggle - Food Nutrition Dataset](https://www.kaggle.com/datasets/shrutisaxena/food-nutrition-dataset?resource=download&select=food.csv)\n",
    "2. [ML_Data_Cleaning_and_Feature_Selection_Abalone.ipynb](https://github.com/aiskunks/YouTube/blob/main/A_Crash_Course_in_Statistical_Learning/ML_Data_Cleaning_and_Feature_Selection/ML_Data_Cleaning_and_Feature_Selection_Abalone.ipynb)\n",
    "3. [USDA Food Data Bank](https://fdc.nal.usda.gov/index.html)\n",
    "4. [NIH Fact Sheet](https://ods.od.nih.gov/factsheets/VitaminA-HealthProfessional/#:~:text=The%20units%20of%20measurement%20for,beta%2Dcarotene%20%3D%200.3%20mcg%20RAE)\n",
    "5. [Feature Selection Techniques - Vidhya.com](https://www.analyticsvidhya.com/blog/2020/10/feature-selection-techniques-in-machine-learning/)\n",
    "6. [Investopedia](https://www.investopedia.com/terms/d/durbin-watson-statistic.asp)\n",
    "7. [Imputation Methods](https://www.youtube.com/watch?v=fYhr8eF1ubo)\n",
    "8.[ML_Data_Cleaning_and_Feature_Selection_Breast_Cancer.ipynb](https://github.com/nikbearbrown/INFO_6105_Data_Science_Engineering_Methods/blob/sorting_branch/Assignment_1/ML_Data_Cleaning_and_Feature_Selection_Breast_Cancer.ipynb)\n",
    "9. [MLData_Cleaning_and_Feature_Selection_NY_Airbnb.ipynb](https://github.com/nikbearbrown/INFO_6105_Data_Science_Engineering_Methods/blob/sorting_branch/Assignment_1/MLData_Cleaning_and_Feature_Selection_NY_Airbnb.ipynb)\n",
    "10. [ML Data Cleaning & Feature Selection using Telecom Client Churn Dataset](https://github.com/nikbearbrown/INFO_6105_Data_Science_Engineering_Methods/blob/sorting_branch/Assignment_1/ML%20Data%20Cleaning%20and%20Feature%20Selection.ipynb)\n",
    "11. [6105_ML_Data_Cleaning_and_Feature_WineQuality.ipynb](https://github.com/nikbearbrown/INFO_6105_Data_Science_Engineering_Methods/blob/sorting_branch/Assignment_1/6105_ML_Data_Cleaning_and_Feature_WineQuality.ipynb)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T01:14:25.888900Z",
     "start_time": "2023-09-29T01:14:22.296807Z"
    }
   },
   "id": "f5457a0f846d0666"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
